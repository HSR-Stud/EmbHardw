\section{Multi-Processor Scheduling \weekDoran{5}}
	\subsection{Amdahls Law}
		Amdahls Law	describes the potential possible speed up, if a part of the code is parallelized by using a multi processor system.
		\begin{longtable}{p{0.6\linewidth}p{0.35\linewidth}}
			\vspace{0pt}
			
			\includegraphics[width=0.6\textwidth]{./pictures/amdahl.png}
			&
			\begin{equation*}
				\begin{aligned}
					S(n) &= \frac{1}{1-p-\frac{p}{n}} \\
					S(n) &\leq  \frac{1}{1-p} \\
					S &= \text{Speedup} \\
					p &= \text{Parallel Portion} \\
					n &= \text{Number of Processors} \\
					S(n) &= \frac{T}{t_s+\frac{t_p}{n}} \\
					S(n) &\leq  \frac{T}{T-t_p}=\frac{T}{t_s} \\
					T &= \text{Total execution time} \\
					t_p &= \text{Parallel execution time} \\
					t_s &= \text{Serial execution time} \\
				\end{aligned}
			\end{equation*}
		\end{longtable}

	\subsection{TAS (Test and Set) and TTAS (Test and Test and Set)}
		\textbf{TAS:} Atomic instruction, noninterruptible; slow because of bus locking and cache invalidation \\
		\textbf{TTAS:} Only calls TAS when it is not already locked\\
		\textbf{Example:} Each thread has it's own cache. Cache is updated only when the variable is marked as dirty. TAS (at the OS level) marks the variable as dirty whenever it's called, regardless of whether it succeeded in setting the value or not. This is what causes a big overhead. Because of that, all threads keep invalidating the cache all the time. In TTAS case, you avoid calling TAS as much and therefore only invalidate cache on, now much rarer, TAS calls and when you release the lock.
		
	\subsection{Scheduling}
		\begin{longtable}{|>{\bfseries}p{0.1\linewidth}|p{0.45\linewidth}|p{0.4\linewidth}|}
			\hline
			SQMS
			&
				\begin{compactitem}
					\item Single Queue Multiprocessor Scheduling
					\item With a simple task queue and no special arrangements, the schedule such as the middle picture will happen
					\item Every time a task is swapped to another CPU the cache has to be reloaded.
				\end{compactitem}
			&
			\vspace{0pt}
			
			\includegraphics[width=\linewidth]{./pictures/sqms.png} \\
			\hline
			MQMS
			&
			\begin{compactitem}
				\item Multi Queue Multiprocessor Scheduling
				\item Scales better as lock and cache contention prevented and cache affinity provided
				\item Suffers from load-imbalancing $\rightarrow$ A gets too much processing time, whereas B and D have to share. If A stops then B and D still share the same CPU and CPU 0 is idle.
			\end{compactitem}
			&
			\vspace{0pt}
			
			\includegraphics[width=\linewidth]{./pictures/mqms.png}\\
			\hline
			Continuous migration \newline\newline
			Work stealing
			&
			\begin{compactitem}
				\item Continuous migration: Cache affinity is partially kept
				\item Work stealing: A queue will look at other queues to see if they have more tasks and takes them if they do
			\end{compactitem}
			&
			\vspace{0pt}
			
			\includegraphics[width=\linewidth]{./pictures/mqms2.png}\\
			\hline
		\end{longtable}
	
	\subsection{Network on Chip}
		There are three essential types of on-chip communication systems: Point to Point (P2P, a), Bus Systems (b) and Network on Chip (c). \\
		\includegraphics[width=\linewidth]{./pictures/network.png}\\
		NOC offer high bandwidth and scaleability as well as portability across platforms.
		
		\subsubsection{Switching}
			\begin{longtable}{|>{\bfseries}p{0.2\linewidth}|p{0.75\linewidth}|}
				\hline
				Circuit switching
				&
				physical path between the source and the destination is reserved prior to the transmission of data \newline
				Advantage: low latency transfers, once path is reserved \newline
				Disadvantage: several links are occupied for the duration of the transmitted data, even when no data is	being transmitted \\
				\hline
				Packet Switching
				&
				packets are transmitted from source and make their way independently to receiver possibly along different routes and with different delays \newline
				Advantage: zero start up time, followed by a variable delay due to contention in routers along packet path \newline
				Disadvantage: QoS guarantees are harder to make in packet switching than in circuit switching\\
				\hline
				SAF switching
				&
				packet is sent from one router to the next only if the receiving router has buffer space for entire	packet \newline
				Disadvantage: excessive buffer requirements \\
				\hline
				VCT Switching
				&
				reduces router latency over SAF switching by forwarding first flit of a packet as soon as space for	the entire packet is available in the next router \newline
				if no space is available in receiving buffer, no flits are sent, and the entire packet is buffered same buffering requirements as SAF switching \\
				\hline
				WH switching
				&
				flit from a packet is forwarded to receiving router if space exists for that flit \newline
				Advantage: buffer requirements are reduced to one flit, instead of an entire packet \newline
				Disadvantage: more susceptible to deadlocks due to usage dependencies between links \\
				\hline
			\end{longtable}
		
	\subsection{Testing}
		\begin{longtable}{|>{\bfseries}p{0.2\linewidth}|p{0.75\linewidth}|}
			\hline
			Black Box testing 
			& Behavioral model required, so called golden model \\
			\hline
			White Box testing
			& DUT interna is known and this knowledge is taken into account during the test \\
			\hline
			Gray Box testing
			& A combination of Black and White Box testing \\
			\hline
		\end{longtable}